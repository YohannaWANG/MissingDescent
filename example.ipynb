{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2YmMecGJRQIY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.optimize import minimize\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "from tqdm import tqdm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5oh3S7t6rLx",
    "outputId": "98f45363-2ccf-47d0-c25d-43345689bf0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.7.0\n",
      "Use ***GPU***\n",
      "31.74853515625 GB\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "  print('Use ***GPU***')\n",
    "  print(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024,'GB')\n",
    "else:\n",
    "  print('Use CPU')\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mSdqwDGuCAC",
    "outputId": "52424a9b-0156-4772-d70f-90f9299669f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       nan, 3.69936727,        nan, ..., 3.73016296, 0.72552897,\n",
       "               nan],\n",
       "       [       nan,        nan,        nan, ..., 4.09845224, 3.98666281,\n",
       "        1.4010849 ],\n",
       "       [1.4172262 , 4.55975095,        nan, ..., 1.45969442, 0.21816242,\n",
       "        1.10356968],\n",
       "       ...,\n",
       "       [       nan,        nan,        nan, ...,        nan, 3.33037147,\n",
       "               nan],\n",
       "       [       nan, 6.00709234,        nan, ..., 1.33762049,        nan,\n",
       "               nan],\n",
       "       [       nan, 5.47661132,        nan, ..., 2.78265456,        nan,\n",
       "        3.35730083]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Generate synthetic data \"\"\"\n",
    "np.random.seed(2022)\n",
    "\n",
    "N = 50000\n",
    "d = 100\n",
    "mu = np.random.uniform(5,10, d)\n",
    "Sig_factor = np.random.uniform(0,1, (d,20))\n",
    "Sigma = Sig_factor.dot(Sig_factor.T)+np.eye(d)\n",
    "X = np.random.multivariate_normal(mu, Sigma, N)\n",
    "\n",
    "# Truncate\n",
    "percentile = -0.383 # 0.675 => remove 25%, 0.0 => remove 0.5%, -0.675=> remove 75%\n",
    "std = np.sqrt(np.diag(Sigma))\n",
    "cond_lower = mu + percentile*std\n",
    "cond_upper = [10000.0]*d\n",
    "for i in range(X.shape[1]):\n",
    "  X[np.logical_and(X[:,i]>cond_lower[i], X[:,i]<cond_upper[i]),i] = np.nan\n",
    "X[:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "c14DMpOR0dfk"
   },
   "outputs": [],
   "source": [
    "class MeanEst():\n",
    "  def __init__(self):\n",
    "    self.affine_transform = True\n",
    "\n",
    "  def PairwiseEmp(self, X, pair_idx, M, batch_size):\n",
    "    mu_hatS_lst = []\n",
    "    Sig_hatS_lst = []\n",
    "    O_aff_lst = []\n",
    "    rand_sample_idx = []\n",
    "    n_lst = []\n",
    "    for i in range(X.shape[1]):\n",
    "      O = X[pair_idx[str(i)],:][:,[i]]\n",
    "\n",
    "      mu_hatS = torch.mean(O,axis=0)\n",
    "      X_pair = O - mu_hatS\n",
    "      n, d = X_pair.shape[0], 1\n",
    "      A = X_pair.reshape(n,d,1)\n",
    "      B = torch.transpose(A, dim0=2, dim1=1)\n",
    "      # Sig_hatS = torch.mean(torch.einsum('ijk,ikd->ijd', A, B),axis=0)\n",
    "      Sig_hatS = torch.mean(torch.matmul(A, B),axis=0)\n",
    "\n",
    "      if self.affine_transform==True:\n",
    "        O_aff = (1/torch.sqrt(Sig_hatS)).matmul(X_pair.t()).t()\n",
    "      else:\n",
    "        O_aff = O\n",
    "\n",
    "      mu_hatS_lst.append(mu_hatS.reshape(-1,1))\n",
    "      Sig_hatS_lst.append(Sig_hatS)\n",
    "      O_aff_lst.append(O_aff)\n",
    "      rand_sample_idx.append(np.random.choice(range(0,n), (M,batch_size)))\n",
    "      n_lst.append(n)\n",
    "\n",
    "    n_max = np.max(n_lst)\n",
    "\n",
    "    for i in range(len(O_aff_lst)):\n",
    "      n = n_lst[i]\n",
    "      nan_tensor = torch.zeros((n_max-n,1))\n",
    "      nan_tensor[:,:] = float('nan')\n",
    "      O_aff_lst[i] = torch.cat((O_aff_lst[i], nan_tensor), axis=0)\n",
    "\n",
    "    return torch.stack(mu_hatS_lst), torch.stack(Sig_hatS_lst), torch.stack(O_aff_lst), np.stack(rand_sample_idx).transpose((1,0,2))#np.stack(rand_sample_idx).T\n",
    "\n",
    "  def Pair_IDX(self, X, cond_lower, cond_upper):\n",
    "    pair_idx = {}\n",
    "    cond_lower_lst = []\n",
    "    cond_upper_lst = []\n",
    "    for i in range(X.shape[1]):\n",
    "        pair_idx[str(i)] = torch.where(~torch.isnan(X[:,i])==True)[0]\n",
    "        cond_lower_lst.append(torch.tensor([cond_lower[i]]))\n",
    "        cond_upper_lst.append(torch.tensor([cond_upper[i]]))\n",
    "    \n",
    "    return pair_idx, torch.stack(cond_lower_lst), torch.stack(cond_upper_lst)\n",
    "\n",
    "  def SGD(self, M, batch_size, lamb, X, cond_lower, cond_upper, show_progress_bar=True, display_per_iter=100):\n",
    "\n",
    "    pair_idx, pair_cond_lower, pair_cond_upper = self.Pair_IDX(X, cond_lower, cond_upper)\n",
    "    mu_hatS_lst, Sig_hatS_lst, O_aff_lst, rand_sample_idx = self.PairwiseEmp(X, pair_idx, M, batch_size)\n",
    "#     print(mu_hatS_lst,Sig_hatS_lst)\n",
    "    # print(O_aff_lst.shape)\n",
    "    if self.affine_transform==True:\n",
    "      w = [torch.stack([torch.cat((torch.eye(1).reshape(-1), torch.zeros(1))) for i in range(len(pair_idx))])]\n",
    "    else:\n",
    "      T_hatS_lst = 1/Sig_hatS_lst\n",
    "      v_hatS_lst = T_hatS_lst.matmul(mu_hatS_lst)\n",
    "      w = [torch.cat((T_hatS_lst.reshape((Sig_hatS_lst.shape[0],4)), v_hatS_lst.reshape((mu_hatS_lst.shape[0],2))), dim=1)]\n",
    "    # print(w)\n",
    "    # print(mu_hatS_lst, Sig_hatS_lst)\n",
    "    grad_lst = []\n",
    "    \n",
    "    if show_progress_bar==True:\n",
    "      loop_list = tqdm(range(1, M+1))\n",
    "    else:\n",
    "      loop_list = range(1, M+1)\n",
    "    \n",
    "    for i in loop_list:\n",
    "      idx = rand_sample_idx[i-1].T\n",
    "      x = O_aff_lst[range(0, O_aff_lst.shape[0]),idx,:].transpose(dim0=1,dim1=0).transpose(dim0=2,dim1=1)\n",
    "      \n",
    "      nu = 1/lamb\n",
    "      grad = self.GradientEstimation(x, w[-1], mu_hatS_lst, Sig_hatS_lst, pair_cond_lower, pair_cond_upper, batch_size)\n",
    "      # print(grad.shape)\n",
    "      grad_lst.append(torch.sum(grad[0,:]**2))\n",
    "      r = w[-1] - nu*grad\n",
    "      # w.append(self.ProjectToDomain(r, r_star))\n",
    "      w.append(r)\n",
    "\n",
    "      if show_progress_bar==False and (i+1)%display_per_iter==0:\n",
    "        print('Iteration:', i+1)\n",
    "\n",
    "    w = torch.stack(w)\n",
    "    N = min(int(w.shape[0]/2), 1000)\n",
    "    w_bar = torch.mean(w[-N:],axis=0)\n",
    "    T_bar_lst = w_bar[:,:1].reshape((w_bar.shape[0], 1, 1))\n",
    "    v_bar_lst = w_bar[:,1:].reshape((w_bar.shape[0], 1, 1))\n",
    "\n",
    "    Sig_bar = 1/T_bar_lst\n",
    "    mu_bar = Sig_bar.matmul(v_bar_lst)\n",
    "\n",
    "    if self.affine_transform==True:\n",
    "      Sig_hatS_half = torch.sqrt(Sig_hatS_lst)\n",
    "\n",
    "      Sig_hat = Sig_hatS_half.matmul(Sig_bar).matmul(torch.transpose(Sig_hatS_half, dim0=2,dim1=1))\n",
    "      mu_hat = Sig_hatS_half.matmul(mu_bar) + mu_hatS_lst\n",
    "    else:\n",
    "      Sig_hat = Sig_bar\n",
    "      mu_hat = mu_bar\n",
    "\n",
    "    return (Sig_hat, mu_hat, torch.stack(grad_lst), w)\n",
    "\n",
    "  def MS(self, y, mu_hatS, Sig_hatS, pair_cond_lower, pair_cond_upper):\n",
    "    if self.affine_transform==True:\n",
    "      Sig_hatS_half = torch.sqrt(Sig_hatS)\n",
    "      y_prime = Sig_hatS_half.matmul(y) + mu_hatS#.reshape((y.shape[0], y.shape[1]))\n",
    "    else:\n",
    "      y_prime = y\n",
    "    # print(y_prime.shape, pair_cond_lower.shape)\n",
    "    pair_cond_lower = pair_cond_lower.reshape((pair_cond_lower.shape[0],pair_cond_lower.shape[1],1))\n",
    "    pair_cond_upper = pair_cond_upper.reshape((pair_cond_upper.shape[0],pair_cond_upper.shape[1],1))\n",
    "    cond = torch.logical_and(y_prime > pair_cond_lower, y_prime < pair_cond_upper)\n",
    "    # cond = ~torch.logical_or(cond[:,0,:], cond[:,1,:])\n",
    "    cond = ~cond[:,0,:]\n",
    "    idx_accept = torch.where(cond==True)\n",
    "    return idx_accept\n",
    "    \n",
    "  def GradientEstimation(self, x, w, mu_hatS, Sig_hatS, pair_cond_lower, pair_cond_upper, batch_size):\n",
    "    T_lst, v_lst = w[:,:1].reshape((w.shape[0], 1, 1)), w[:,1:].reshape(w.shape[0],1,1)\n",
    "    Sigma = 1/T_lst\n",
    "    Sigma_half = torch.sqrt(Sigma)\n",
    "    mu = Sigma.matmul(v_lst)\n",
    "\n",
    "    y = torch.zeros((w.shape[0], 1, batch_size))\n",
    "    marker = torch.zeros(w.shape[0], batch_size)\n",
    "    # print(pair_cond_lower, pair_cond_upper)\n",
    "\n",
    "    while True:\n",
    "      y_sample = mu + Sigma_half.matmul(torch.randn((w.shape[0],1,batch_size)))\n",
    "      # print(y_sample)\n",
    "      idx_accept_row, idx_accept_row_col = self.MS(y_sample, mu_hatS, Sig_hatS, pair_cond_lower, pair_cond_upper)\n",
    "      if len(idx_accept_row) > 0:\n",
    "        y[idx_accept_row,:,idx_accept_row_col] = y_sample[idx_accept_row,:,idx_accept_row_col]\n",
    "        marker[idx_accept_row, idx_accept_row_col] = 1\n",
    "      if torch.sum(marker)==marker.shape[0]*marker.shape[1]:\n",
    "        break\n",
    "\n",
    "    x = x.transpose(dim0=2,dim1=1)\n",
    "    x = x.reshape((x.shape[0],x.shape[1],1,1))\n",
    "    y = y.transpose(dim0=2,dim1=1)\n",
    "    y = y.reshape((y.shape[0],y.shape[1],1,1))\n",
    "\n",
    "    M = 0.5*torch.mean(x.matmul(torch.transpose(x, dim0=3, dim1=2)),dim=1) - 0.5*torch.mean(y.matmul(torch.transpose(y, dim0=3, dim1=2)),dim=1)\n",
    "    m = -torch.mean(x,dim=1) + torch.mean(y,dim=1)\n",
    "    return torch.cat((M.reshape(w.shape[0],1), m.reshape((w.shape[0],1))),axis=1)\n",
    "\n",
    "  def LearnSigma(self, Sigma_hat_lst, mu_hat_lst, dim):\n",
    "    pair_idx = {}\n",
    "    k = 0\n",
    "    for i in range(dim):\n",
    "      for j in range(i+1,dim):\n",
    "        pair_idx[str(i) + str(j)] = k\n",
    "        k += 1\n",
    "\n",
    "    Sig_learned = cp.Variable((dim, dim), symmetric=True)\n",
    "    Sig_hat_ij = cp.Parameter((int(dim*(dim-1)/2), 4))\n",
    "    # The operator >> denotes matrix inequality.\n",
    "    constraints = [Sig_learned >> 0] # Positive semidefinite constraint\n",
    "    constraints += [\n",
    "        Sig_learned[:,[i,j]][[i,j],:] << (1+0.05)*cp.reshape(Sig_hat_ij[pair_idx[str(i)+str(j)]],(2,2)) for i in range(dim) for j in range(i+1,dim) \n",
    "    ]\n",
    "    constraints += [\n",
    "        Sig_learned[:,[i,j]][[i,j],:] >> (1-0.05)*cp.reshape(Sig_hat_ij[pair_idx[str(i)+str(j)]], (2,2)) for i in range(dim) for j in range(i+1,dim) \n",
    "    ]\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(0), constraints)\n",
    "    # prob.solve()\n",
    "    cvxpylayer = CvxpyLayer(prob, parameters=[Sig_hat_ij], variables=[Sig_learned])\n",
    "\n",
    "    # solve the problem\n",
    "    solution, = cvxpylayer(Sigma_hat_lst.reshape(-1,4))\n",
    "\n",
    "    # Print result.\n",
    "    # print(\"A solution X is\")\n",
    "    # print(solution)\n",
    "    return solution\n",
    "\n",
    "  def Train(self, M, batch_size, lamb, X, cond_lower, cond_upper,\n",
    "            affine_transform=True, show_progress_bar=True, display_per_iter=100):\n",
    "    self.affine_transform = affine_transform\n",
    "    Sigma_hat_lst, mu_hat_lst, grad, w  = self.SGD(M, batch_size, lamb, X, cond_lower, cond_upper,\n",
    "                                                   show_progress_bar=show_progress_bar,\n",
    "                                                   display_per_iter=display_per_iter)\n",
    "    # Sigma_learned = self.LearnSigma(X, Sigma_hat_lst, mu_hat_lst)\n",
    "    return Sigma_hat_lst, mu_hat_lst, grad, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "efK1Wd9PeFmP"
   },
   "outputs": [],
   "source": [
    "class CovarianceEst():\n",
    "  def __init__(self):\n",
    "    self.affine_transform = True\n",
    "    self.rand_batch_idx = np.vectorize(self.rand_one_batch_idx, signature='(),(),()->(m,k)')\n",
    "\n",
    "  def rand_one_batch_idx(self, n, M, batch_size):\n",
    "    return np.random.choice(range(0,n), (M,batch_size))\n",
    "\n",
    "  def PairwiseEmp(self, X, pair_idx, M, batch_size, init_mu=None, init_var=None):\n",
    "    mu_hatS_lst = []\n",
    "    Sig_hatS_lst = []\n",
    "    O_aff_lst = []\n",
    "    mu_init_lst = []\n",
    "    var_init_lst = []\n",
    "    # rand_sample_idx = []\n",
    "    n_lst = []\n",
    "    for i in range(X.shape[1]):\n",
    "      for j in range(i+1,X.shape[1]):\n",
    "        O = X[pair_idx[str(i)+str(j)],:][:,[i,j]]\n",
    "\n",
    "        mu_hatS = torch.mean(O,axis=0)\n",
    "        X_pair = O - mu_hatS\n",
    "        n, d = X_pair.shape[0], 2\n",
    "        A = X_pair.reshape(n,d,1)\n",
    "        B = torch.transpose(A, dim0=2, dim1=1)\n",
    "        # Sig_hatS = torch.mean(torch.einsum('ijk,ikd->ijd', A, B),axis=0)\n",
    "        Sig_hatS = torch.mean(torch.matmul(A, B),axis=0)\n",
    "\n",
    "        if self.affine_transform==True:\n",
    "          O_aff = torch.inverse(torch.cholesky(Sig_hatS)).matmul(X_pair.t()).t()\n",
    "        else:\n",
    "          O_aff = O\n",
    "\n",
    "        mu_hatS_lst.append(mu_hatS.reshape(-1,1))\n",
    "        Sig_hatS_lst.append(Sig_hatS)\n",
    "        O_aff_lst.append(O_aff)\n",
    "        \n",
    "        if init_mu != None:\n",
    "          mu_init_lst.append(torch.tensor([init_mu[i], init_mu[j]]).reshape(-1,1))\n",
    "        if init_var != None:\n",
    "          var_init_lst.append(torch.tensor([init_var[i], init_var[j]]).reshape(-1,1))\n",
    "        # rand_sample_idx.append(np.random.choice(range(0,n), (M,batch_size)))\n",
    "        n_lst.append(n)\n",
    "\n",
    "    n_max = np.max(n_lst)\n",
    "\n",
    "    for i in range(len(O_aff_lst)):\n",
    "      n = n_lst[i]\n",
    "      nan_tensor = torch.zeros((n_max-n,2))\n",
    "      nan_tensor[:,:] = float('nan')\n",
    "      O_aff_lst[i] = torch.cat((O_aff_lst[i], nan_tensor), axis=0)\n",
    "\n",
    "      \n",
    "    r = [torch.stack(mu_hatS_lst), torch.stack(Sig_hatS_lst), torch.stack(O_aff_lst), n_lst]\n",
    "    if init_mu != None:\n",
    "      r = r + [torch.stack(mu_init_lst)]\n",
    "    else:\n",
    "      r = r + [None]\n",
    "    if init_var != None:\n",
    "      r = r + [torch.stack(var_init_lst)]\n",
    "    else:\n",
    "      r = r + [None]\n",
    "    return r #np.stack(rand_sample_idx).transpose((1,0,2))#np.stack(rand_sample_idx).T\n",
    "\n",
    "  def Pair_IDX(self, X, cond_lower, cond_upper):\n",
    "    pair_idx = {}\n",
    "    cond_lower_lst = []\n",
    "    cond_upper_lst = []\n",
    "    for i in range(X.shape[1]):\n",
    "      for j in range(i+1,X.shape[1]):\n",
    "        pair_idx[str(i) + str(j)] = torch.where(torch.logical_and(~torch.isnan(X[:,i]), ~torch.isnan(X[:,j]))==True)[0]\n",
    "        cond_lower_lst.append(torch.tensor([cond_lower[i], cond_lower[j]]))\n",
    "        cond_upper_lst.append(torch.tensor([cond_upper[i], cond_upper[j]]))\n",
    "    \n",
    "    return pair_idx, torch.stack(cond_lower_lst), torch.stack(cond_upper_lst)\n",
    "\n",
    "  def SGD(self, M, batch_size, lamb, X, cond_lower, cond_upper,\n",
    "          init_mu=None, init_var=None, show_progress_bar=True, display_per_iter=100):\n",
    "\n",
    "    pair_idx, pair_cond_lower, pair_cond_upper = self.Pair_IDX(X, cond_lower, cond_upper)\n",
    "    mu_hatS_lst, Sig_hatS_lst, O_aff_lst, n_lst, mu_init_lst, var_init_lst = self.PairwiseEmp(X, pair_idx, M, batch_size, init_mu, init_var)\n",
    "#     print(mu_hatS_lst,Sig_hatS_lst)\n",
    "    # print(O_aff_lst.shape)\n",
    "    if self.affine_transform==True:\n",
    "#       w = [torch.stack([torch.cat((torch.eye(2).reshape(-1), torch.zeros(2))) for i in range(len(pair_idx))])]\n",
    "      if init_var==None:\n",
    "        T_hatS_lst = torch.stack([torch.eye(2) for i in range(len(pair_idx))])\n",
    "      else:\n",
    "        Sig_hatS_half = torch.cholesky(Sig_hatS_lst)\n",
    "        Sig_init = Sig_hatS_lst*(1-torch.eye(2)) + torch.eye(2)*var_init_lst\n",
    "        Sig_init_aff = torch.inverse(Sig_hatS_half).matmul(Sig_init).matmul(torch.inverse(torch.transpose(Sig_hatS_half, dim0=2,dim1=1)))\n",
    "        T_hatS_lst = torch.inverse(Sig_init_aff)\n",
    "        \n",
    "      if init_mu==None:\n",
    "        v_hatS_lst = torch.stack([torch.zeros(2) for i in range(len(pair_idx))])\n",
    "      else:\n",
    "        Sig_hatS_half = torch.cholesky(Sig_hatS_lst)\n",
    "        m_init = torch.inverse(Sig_hatS_half).matmul(mu_init_lst - mu_hatS_lst)\n",
    "        v_hatS_lst = T_hatS_lst.matmul(m_init)\n",
    "      w = [torch.cat((T_hatS_lst.reshape((Sig_hatS_lst.shape[0],4)), v_hatS_lst.reshape((mu_hatS_lst.shape[0],2))), dim=1)]\n",
    "    else:\n",
    "      if init_var==None:\n",
    "        T_hatS_lst = torch.inverse(Sig_hatS_lst)\n",
    "      else:\n",
    "        T_hatS_lst = torch.inverse(Sig_hatS_lst*(1-torch.eye(2)) + torch.eye(2)*var_init_lst)\n",
    "        \n",
    "      if init_mu==None:\n",
    "        v_hatS_lst = T_hatS_lst.matmul(mu_hatS_lst)\n",
    "      else:\n",
    "        v_hatS_lst = T_hatS_lst.matmul(mu_init_lst)\n",
    "      w = [torch.cat((T_hatS_lst.reshape((Sig_hatS_lst.shape[0],4)), v_hatS_lst.reshape((mu_hatS_lst.shape[0],2))), dim=1)]\n",
    "    # print(w)\n",
    "    # print(mu_hatS_lst, Sig_hatS_lst)\n",
    "    M_mini = 5000\n",
    "    rand_sample_idx = self.rand_batch_idx(n_lst, M=M_mini, batch_size=batch_size).transpose((1,0,2))\n",
    "    grad_lst = []\n",
    "    \n",
    "    if show_progress_bar==True:\n",
    "      loop_list = tqdm(range(1, M+1))\n",
    "    else:\n",
    "      loop_list = range(1, M+1)\n",
    "    \n",
    "    for i in loop_list:\n",
    "      idx = rand_sample_idx[(i-1) % M_mini].T\n",
    "      if i == M_mini:\n",
    "        rand_sample_idx = None # clear memory\n",
    "        rand_sample_idx = self.rand_batch_idx(n_lst, M=M_mini, batch_size=batch_size).transpose((1,0,2))\n",
    "      # idx = self.rand_batch_idx(n_lst, batch_size).T\n",
    "\n",
    "      x = O_aff_lst[range(0, O_aff_lst.shape[0]),idx,:].transpose(dim0=1,dim1=0).transpose(dim0=2,dim1=1)\n",
    "      \n",
    "      nu = 1/lamb\n",
    "      grad = self.GradientEstimation(x, w[-1], mu_hatS_lst, Sig_hatS_lst, pair_cond_lower, pair_cond_upper, batch_size)\n",
    "      # print(grad.shape)\n",
    "      grad_lst.append(torch.sum(grad[0,:]**2))\n",
    "      r = w[-1] - nu*grad\n",
    "      # w.append(self.ProjectToDomain(r, r_star))\n",
    "      w.append(r)\n",
    "\n",
    "      if show_progress_bar==False and (i+1)%display_per_iter==0:\n",
    "        print('Iteration:', i+1)\n",
    "\n",
    "    w = torch.stack(w)\n",
    "    N = min(int(w.shape[0]/2), 1000)\n",
    "    w_bar = torch.mean(w[-N:],axis=0)\n",
    "    T_bar_lst = w_bar[:,:4].reshape((w_bar.shape[0], 2, 2))\n",
    "    v_bar_lst = w_bar[:,4:].reshape((w_bar.shape[0], 2, 1))\n",
    "\n",
    "    Sig_bar = torch.inverse(T_bar_lst)\n",
    "    mu_bar = Sig_bar.matmul(v_bar_lst)\n",
    "\n",
    "    if self.affine_transform==True:\n",
    "      Sig_hatS_half = torch.cholesky(Sig_hatS_lst)\n",
    "\n",
    "      Sig_hat = Sig_hatS_half.matmul(Sig_bar).matmul(torch.transpose(Sig_hatS_half, dim0=2,dim1=1))\n",
    "      mu_hat = Sig_hatS_half.matmul(mu_bar) + mu_hatS_lst\n",
    "    else:\n",
    "      Sig_hat = Sig_bar\n",
    "      mu_hat = mu_bar\n",
    "\n",
    "    return (Sig_hat, mu_hat, torch.stack(grad_lst), w)\n",
    "\n",
    "  def MS(self, y, mu_hatS, Sig_hatS, pair_cond_lower, pair_cond_upper):\n",
    "    if self.affine_transform==True:\n",
    "      Sig_hatS_half = torch.cholesky(Sig_hatS)\n",
    "      y_prime = Sig_hatS_half.matmul(y) + mu_hatS#.reshape((y.shape[0], y.shape[1]))\n",
    "    else:\n",
    "      y_prime = y\n",
    "    # print(y_prime.shape, pair_cond_lower.shape)\n",
    "    pair_cond_lower = pair_cond_lower.reshape((pair_cond_lower.shape[0],pair_cond_lower.shape[1],1))\n",
    "    pair_cond_upper = pair_cond_upper.reshape((pair_cond_upper.shape[0],pair_cond_upper.shape[1],1))\n",
    "    cond = torch.logical_and(y_prime > pair_cond_lower, y_prime < pair_cond_upper)\n",
    "    cond = ~torch.logical_or(cond[:,0,:], cond[:,1,:])\n",
    "    idx_accept = torch.where(cond==True)\n",
    "    return idx_accept\n",
    "    \n",
    "  def GradientEstimation(self, x, w, mu_hatS, Sig_hatS, pair_cond_lower, pair_cond_upper, batch_size):\n",
    "    T_lst, v_lst = w[:,:4].reshape((w.shape[0], 2, 2)), w[:,4:].reshape(w.shape[0],2,1)\n",
    "    Sigma = torch.inverse(T_lst)\n",
    "    Sigma_half = torch.cholesky(Sigma)\n",
    "    mu = Sigma.matmul(v_lst)\n",
    "\n",
    "    y = torch.zeros((w.shape[0], 2, batch_size))\n",
    "    marker = torch.zeros(w.shape[0], batch_size)\n",
    "    # print(pair_cond_lower, pair_cond_upper)\n",
    "\n",
    "    while True:\n",
    "      y_sample = mu + Sigma_half.matmul(torch.randn((w.shape[0],2,batch_size)))\n",
    "      # print(y_sample)\n",
    "      idx_accept_row, idx_accept_row_col = self.MS(y_sample, mu_hatS, Sig_hatS, pair_cond_lower, pair_cond_upper)\n",
    "      if len(idx_accept_row) > 0:\n",
    "        y[idx_accept_row,:,idx_accept_row_col] = y_sample[idx_accept_row,:,idx_accept_row_col]\n",
    "        marker[idx_accept_row, idx_accept_row_col] = 1\n",
    "      if torch.sum(marker)==marker.shape[0]*marker.shape[1]:\n",
    "        break\n",
    "\n",
    "    x = x.transpose(dim0=2,dim1=1)\n",
    "    x = x.reshape((x.shape[0],x.shape[1],2,1))\n",
    "    y = y.transpose(dim0=2,dim1=1)\n",
    "    y = y.reshape((y.shape[0],y.shape[1],2,1))\n",
    "\n",
    "    M = 0.5*torch.mean(x.matmul(torch.transpose(x, dim0=3, dim1=2)),dim=1) - 0.5*torch.mean(y.matmul(torch.transpose(y, dim0=3, dim1=2)),dim=1)\n",
    "    m = -torch.mean(x,dim=1) + torch.mean(y,dim=1)\n",
    "    return torch.cat((M.reshape(w.shape[0],4), m.reshape((w.shape[0],2))),axis=1)\n",
    "\n",
    "  def LearnSigma(self, Sigma_hat_lst, mu_hat_lst, dim):\n",
    "    pair_idx = {}\n",
    "    k = 0\n",
    "    for i in range(dim):\n",
    "      for j in range(i+1,dim):\n",
    "        pair_idx[str(i) + str(j)] = k\n",
    "        k += 1\n",
    "\n",
    "    Sig_learned = cp.Variable((dim, dim), symmetric=True)\n",
    "    Sig_hat_ij = cp.Parameter((int(dim*(dim-1)/2), 4))\n",
    "    # The operator >> denotes matrix inequality.\n",
    "    constraints = [Sig_learned >> 0] # Positive semidefinite constraint\n",
    "    constraints += [\n",
    "        Sig_learned[:,[i,j]][[i,j],:] << (1+0.05)*cp.reshape(Sig_hat_ij[pair_idx[str(i)+str(j)]],(2,2)) for i in range(dim) for j in range(i+1,dim) \n",
    "    ]\n",
    "    constraints += [\n",
    "        Sig_learned[:,[i,j]][[i,j],:] >> (1-0.05)*cp.reshape(Sig_hat_ij[pair_idx[str(i)+str(j)]], (2,2)) for i in range(dim) for j in range(i+1,dim) \n",
    "    ]\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(0), constraints)\n",
    "    # prob.solve()\n",
    "    cvxpylayer = CvxpyLayer(prob, parameters=[Sig_hat_ij], variables=[Sig_learned])\n",
    "\n",
    "    # solve the problem\n",
    "    solution, = cvxpylayer(Sigma_hat_lst.reshape(-1,4))\n",
    "\n",
    "    # Print result.\n",
    "    # print(\"A solution X is\")\n",
    "    # print(solution)\n",
    "    return solution\n",
    "\n",
    "  def Train(self, M, batch_size, lamb, X, cond_lower, cond_upper, init_mu=None, init_var=None, \n",
    "            affine_transform=True, show_progress_bar=True, display_per_iter=100):\n",
    "    self.affine_transform = affine_transform\n",
    "    Sigma_hat_lst, mu_hat_lst, grad, w  = self.SGD(M, batch_size, lamb, X, cond_lower, cond_upper,\n",
    "                                                   init_mu=init_mu, init_var=init_var, \n",
    "                                                   show_progress_bar=show_progress_bar,\n",
    "                                                   display_per_iter=display_per_iter)\n",
    "    # Sigma_learned = self.LearnSigma(X, Sigma_hat_lst, mu_hat_lst)\n",
    "    return Sigma_hat_lst, mu_hat_lst, grad, w\n",
    "\n",
    "  def learn_v_prime(self, v, r2):\n",
    "    b = cp.Variable((1,2))\n",
    "    v_parms = cp.Parameter(v.shape)\n",
    "    constraints = [cp.constraints.nonpos.NonPos(cp.norm(b,2) - r2)]\n",
    "    obj = cp.sum_squares(b - v_parms)\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(obj), constraints)\n",
    "    cvxpylayer = CvxpyLayer(prob, parameters=[v_parms], variables=[b])\n",
    "\n",
    "    solution, = cvxpylayer(v)\n",
    "\n",
    "    return solution\n",
    "\n",
    "  def learn_T_prime(self, T, r3, lamb):\n",
    "    T_prime = cp.Variable((2,2))\n",
    "    T_parms = cp.Parameter(T.shape)\n",
    "    I_parms = cp.Parameter(T.shape)\n",
    "    constraints = [T_prime>>r3*I_parms]\n",
    "    obj = cp.sum_squares(T_prime - T_parms) + lamb*cp.sum_squares(I_parms - T_prime)\n",
    "    prob = cp.Problem(cp.Minimize(obj), constraints)\n",
    "    cvxpylayer = CvxpyLayer(prob, parameters=[T_parms, I_parms], variables=[T_prime])\n",
    "    I = torch.eye(2)\n",
    "    solution, = cvxpylayer(T, I)\n",
    "\n",
    "    opt_val = torch.sum((solution - T)**2) + lamb*torch.sum((I - solution)**2)\n",
    "\n",
    "    return solution, opt_val\n",
    "\n",
    "  def ProjectToDomain(self, r, r_star):\n",
    "    T, v = r[:,:4].reshape(2,2), r[:,4:]\n",
    "    r1, r2, r3 = r_star, r_star, 1/r_star\n",
    "\n",
    "    v_prime = self.learn_v_prime(v, r1)\n",
    "\n",
    "    lamb = 1\n",
    "    cond_lst = []\n",
    "    T_prime_lst = []\n",
    "    opt_val_lst = []\n",
    "    for i in range(10):\n",
    "      T_prime, opt_value = self.learn_T_prime(T,r3,lamb)\n",
    "\n",
    "      cond = torch.sum((torch.eye(2)-T_prime)**2)\n",
    "      cond_lst.append(cond)\n",
    "      T_prime_lst.append(T_prime)\n",
    "      opt_val_lst.append(opt_value)\n",
    "\n",
    "      lamb = lamb/2\n",
    "\n",
    "    cond_lst = torch.tensor(cond_lst)\n",
    "    T_prime_lst = torch.stack(T_prime_lst)\n",
    "    opt_val_lst = torch.tensor(opt_val_lst)\n",
    "\n",
    "    T_prime_lst = T_prime_lst[cond_lst <= r2**2]\n",
    "    opt_val_lst = opt_val_lst[cond_lst <= r2**2]\n",
    "    \n",
    "    return torch.cat((T_prime_lst[torch.argmin(opt_val_lst)].reshape(-1), v_prime.reshape(-1))).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = torch.load('results-{}-remove65-new.pk'.format(100))\n",
    "# result_mu = load['result_mu']\n",
    "# result_Sigma_pairwise = load['result_Sigma_pairwise']\n",
    "# X = load['X']\n",
    "# time_lst = load['time_lst']\n",
    "# Sigma = load['Sigma']\n",
    "# mu = load['mu']\n",
    "# n_lst = load['n_lst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "n = 15000\n",
      "Estimating mean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:14<00:00, 149.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pairwise covariance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [22:02<00:00, 15.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "tensor([ 5.0257,  6.9017,  5.3363,  4.7734,  7.8312,  7.3383,  9.0452,  8.6448,\n",
      "         8.9825,  8.3310,  9.1193,  9.2085,  9.2471,  9.6825,  7.3334,  7.3873,\n",
      "         6.3421,  8.2149,  9.9437,  5.3418,  8.3746,  6.2964,  6.2709,  8.5677,\n",
      "         5.2540,  7.3302,  9.5344,  6.3629,  5.9395,  7.3837,  8.1390,  6.8046,\n",
      "         9.5642,  5.1645,  9.2745,  7.1878,  8.5990,  9.7412,  6.4934,  9.6476,\n",
      "         5.0823,  4.8375,  6.0336,  6.2266,  7.2363,  7.7834,  8.1152,  7.5512,\n",
      "         4.5843,  9.5410,  4.9970,  8.4487,  4.9807,  6.1315,  6.1050,  6.1412,\n",
      "         9.3796,  4.9521,  6.9443,  8.7838,  5.7889,  5.1711,  8.4960,  5.6724,\n",
      "         7.3418,  8.0002,  9.8549,  4.7497,  4.9974,  9.4722,  6.9898,  9.1607,\n",
      "         7.2183,  7.1643,  9.0428,  7.8438,  8.9188,  5.2857, 10.2037,  6.4945,\n",
      "         6.2233,  8.5762,  6.5860,  5.3851,  4.8851,  8.9260,  7.6934,  9.2051,\n",
      "         7.7850,  5.5883,  6.7491,  5.5079,  9.3117,  6.5832,  5.0511,  9.2812,\n",
      "         8.2339,  5.0952,  5.1900,  5.1593])\n",
      "Pairwise covariances:\n",
      "tensor([[[7.1000, 5.4436],\n",
      "         [5.4436, 7.6161]],\n",
      "\n",
      "        [[6.8985, 4.0392],\n",
      "         [4.0392, 6.3515]],\n",
      "\n",
      "        [[6.6757, 4.2648],\n",
      "         [4.2648, 6.9892]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.3502, 3.3158],\n",
      "         [3.3158, 5.7476]],\n",
      "\n",
      "        [[5.7496, 4.4192],\n",
      "         [4.4192, 8.5052]],\n",
      "\n",
      "        [[6.2110, 5.1614],\n",
      "         [5.1614, 8.9711]]])\n",
      "-----------------------------------\n",
      "n = 25000\n",
      "Estimating mean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:19<00:00, 143.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pairwise covariance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [24:26<00:00, 13.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "tensor([ 5.2879,  7.0071,  5.4602,  5.2183,  8.3164,  7.4338,  9.2202,  8.4122,\n",
      "         9.0362,  8.4847,  9.0164,  9.3784,  9.2511,  9.7444,  7.1280,  7.3954,\n",
      "         6.3968,  8.1563,  9.7299,  5.5503,  8.3082,  6.1032,  6.2367,  8.8142,\n",
      "         5.4546,  7.1891,  9.5154,  6.4479,  6.1818,  7.3196,  7.8750,  6.7381,\n",
      "         9.4564,  5.2505,  9.1549,  7.1033,  8.6703,  9.8016,  6.7545,  9.8298,\n",
      "         5.5304,  5.0096,  6.1270,  6.5534,  7.1531,  8.0834,  8.3504,  7.7741,\n",
      "         4.8262,  9.7389,  5.1077,  8.8288,  4.9906,  6.1753,  6.1985,  6.4178,\n",
      "         9.3698,  5.1686,  7.1179,  9.0694,  6.1941,  5.2297,  8.4894,  5.8171,\n",
      "         7.3842,  8.0859, 10.0429,  4.9191,  5.2495,  9.8040,  7.1640,  9.3402,\n",
      "         7.3477,  7.0928,  9.2705,  7.8408,  8.9012,  5.3987,  9.9461,  6.5303,\n",
      "         6.5661,  8.4564,  6.7066,  5.7074,  4.9801,  8.9509,  8.1354,  9.2042,\n",
      "         7.8273,  5.8488,  6.9719,  6.0963,  9.6634,  6.7755,  5.3738,  9.3347,\n",
      "         8.6885,  5.2742,  5.3696,  5.3027])\n",
      "Pairwise covariances:\n",
      "tensor([[[7.3021, 5.7074],\n",
      "         [5.7074, 7.7933]],\n",
      "\n",
      "        [[8.0022, 5.2991],\n",
      "         [5.2991, 7.7914]],\n",
      "\n",
      "        [[7.1706, 4.7968],\n",
      "         [4.7968, 7.6408]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.4354, 3.5871],\n",
      "         [3.5871, 6.3489]],\n",
      "\n",
      "        [[5.6516, 4.4853],\n",
      "         [4.4853, 8.9640]],\n",
      "\n",
      "        [[6.5074, 5.5074],\n",
      "         [5.5074, 9.3503]]])\n",
      "-----------------------------------\n",
      "n = 30000\n",
      "Estimating mean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:20<00:00, 141.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pairwise covariance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [25:03<00:00, 13.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "tensor([ 5.0337,  7.1568,  5.5562,  5.2415,  8.4246,  7.3717,  9.4622,  8.5096,\n",
      "         8.9394,  8.6534,  9.1926,  9.3098,  9.1831,  9.6328,  7.0463,  7.4187,\n",
      "         6.4843,  8.1375,  9.8726,  5.8260,  8.3507,  6.2510,  6.4302,  8.6603,\n",
      "         5.4989,  7.1926,  9.4479,  6.6095,  6.0809,  7.1823,  8.0022,  7.1325,\n",
      "         9.6535,  5.1682,  9.0435,  7.2814,  8.5906,  9.6495,  6.9495,  9.9296,\n",
      "         5.4684,  5.0802,  6.2729,  6.6485,  7.2502,  8.3092,  8.2299,  7.6878,\n",
      "         4.9124,  9.6250,  5.2579,  8.7751,  4.9975,  6.2431,  5.8576,  6.3116,\n",
      "         9.5366,  4.9743,  7.0815,  9.2522,  6.0287,  5.1697,  8.4981,  6.0497,\n",
      "         7.5131,  8.1455, 10.1575,  4.9190,  5.1387,  9.7662,  7.2635,  9.2186,\n",
      "         6.9458,  7.1591,  9.3987,  7.8255,  8.7799,  5.5772,  9.9773,  6.4169,\n",
      "         6.5855,  8.6547,  6.9584,  5.7071,  5.1579,  8.9134,  8.1507,  9.1831,\n",
      "         7.9879,  5.9563,  7.1715,  6.0050,  9.4654,  6.9028,  5.3962,  9.2791,\n",
      "         8.6124,  5.3609,  5.2149,  5.3019])\n",
      "Pairwise covariances:\n",
      "tensor([[[7.2909, 5.6929],\n",
      "         [5.6929, 7.9614]],\n",
      "\n",
      "        [[7.9562, 4.8940],\n",
      "         [4.8940, 6.9181]],\n",
      "\n",
      "        [[7.7168, 5.2614],\n",
      "         [5.2614, 8.0419]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.6622, 4.1061],\n",
      "         [4.1061, 7.1820]],\n",
      "\n",
      "        [[6.0815, 5.0847],\n",
      "         [5.0847, 9.4436]],\n",
      "\n",
      "        [[6.4799, 5.7304],\n",
      "         [5.7304, 9.9666]]])\n",
      "-----------------------------------\n",
      "n = 35000\n",
      "Estimating mean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:21<00:00, 141.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pairwise covariance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [26:48<00:00, 12.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "tensor([ 5.1588,  7.2435,  5.6445,  5.3433,  8.2479,  7.3149,  9.4901,  8.3363,\n",
      "         9.1169,  8.5422,  9.2244,  9.4326,  9.3421,  9.7264,  7.0883,  7.4325,\n",
      "         6.4675,  8.1817,  9.9635,  5.9687,  8.4217,  6.3501,  6.4076,  8.8083,\n",
      "         5.4704,  7.5094,  9.3144,  6.7290,  6.0980,  7.1901,  8.0624,  6.8913,\n",
      "         9.6741,  5.1960,  9.3330,  6.9632,  8.8523,  9.6338,  6.8518,  9.9145,\n",
      "         5.4081,  5.2157,  6.1559,  6.7264,  7.3183,  8.3181,  8.3349,  7.8086,\n",
      "         4.9210,  9.5499,  5.2440,  9.0901,  5.0633,  6.3499,  5.9461,  6.3374,\n",
      "         9.6139,  4.9383,  6.9611,  9.3583,  5.9299,  5.0268,  8.6298,  6.1741,\n",
      "         7.5156,  8.2204, 10.0875,  5.1075,  5.2777,  9.9386,  7.2112,  9.2470,\n",
      "         7.1222,  7.0051,  9.4271,  7.9911,  8.7151,  5.6695, 10.1906,  6.5669,\n",
      "         6.6398,  8.6090,  7.1250,  5.6500,  5.1587,  8.9684,  8.3168,  9.2190,\n",
      "         8.0439,  6.0897,  7.1588,  6.0375,  9.6662,  6.9935,  5.2497,  9.2132,\n",
      "         8.8320,  5.1625,  5.3676,  5.2319])\n",
      "Pairwise covariances:\n",
      "tensor([[[ 8.5279,  6.9716],\n",
      "         [ 6.9716,  9.1776]],\n",
      "\n",
      "        [[ 8.0530,  5.3608],\n",
      "         [ 5.3608,  7.8464]],\n",
      "\n",
      "        [[ 7.3358,  5.1281],\n",
      "         [ 5.1281,  8.0892]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.3869,  3.9544],\n",
      "         [ 3.9544,  7.1112]],\n",
      "\n",
      "        [[ 6.1323,  5.2161],\n",
      "         [ 5.2161, 10.2015]],\n",
      "\n",
      "        [[ 6.3356,  5.3901],\n",
      "         [ 5.3901,  9.6390]]])\n",
      "-----------------------------------\n",
      "n = 40000\n",
      "Estimating mean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:24<00:00, 138.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pairwise covariance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [26:40<00:00, 12.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "tensor([ 5.2921,  7.3629,  5.6421,  5.3620,  8.3249,  7.0666,  9.4296,  8.3810,\n",
      "         9.2133,  8.6415,  9.1358,  9.4124,  9.1957,  9.6754,  6.9802,  7.4859,\n",
      "         6.5775,  8.1903,  9.8060,  5.7143,  8.3901,  6.2653,  6.3547,  8.7391,\n",
      "         5.3182,  7.6510,  9.1966,  6.6485,  6.1157,  7.1257,  7.9148,  6.8510,\n",
      "         9.7545,  5.1526,  9.0958,  7.0684,  8.6648,  9.7024,  6.8112,  9.9639,\n",
      "         5.4371,  5.2054,  6.4419,  6.6327,  7.4262,  8.2331,  8.2962,  7.7633,\n",
      "         5.0038,  9.6080,  5.2972,  8.9182,  5.0829,  6.2749,  6.0742,  6.2600,\n",
      "         9.6454,  5.0054,  6.9146,  9.2078,  5.8889,  5.0866,  8.5996,  5.9391,\n",
      "         7.4390,  8.0981, 10.0053,  5.2200,  5.1402,  9.8554,  7.1058,  9.3087,\n",
      "         7.2755,  6.9992,  9.4947,  8.0125,  8.8983,  5.7679,  9.8593,  6.5137,\n",
      "         6.6373,  8.6465,  7.1518,  5.7704,  5.0812,  9.0155,  8.4033,  9.2138,\n",
      "         8.1938,  6.1099,  7.0682,  5.9199,  9.6307,  6.9619,  5.3407,  9.3132,\n",
      "         8.7048,  5.1312,  5.5009,  5.2587])\n",
      "Pairwise covariances:\n",
      "tensor([[[7.7666, 6.2315],\n",
      "         [6.2315, 8.3841]],\n",
      "\n",
      "        [[8.0524, 5.2261],\n",
      "         [5.2261, 7.4029]],\n",
      "\n",
      "        [[7.9773, 5.8431],\n",
      "         [5.8431, 9.0625]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[5.5463, 3.2912],\n",
      "         [3.2912, 6.5442]],\n",
      "\n",
      "        [[5.7092, 4.7158],\n",
      "         [4.7158, 9.1479]],\n",
      "\n",
      "        [[6.5439, 5.5728],\n",
      "         [5.5728, 9.5644]]])\n",
      "-----------------------------------\n",
      "n = 45000\n",
      "Estimating mean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:20<00:00, 142.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pairwise covariance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [25:44<00:00, 12.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "tensor([5.3155, 7.4970, 5.5255, 5.4582, 8.3732, 7.1906, 9.4442, 8.5360, 9.0171,\n",
      "        8.6190, 9.0747, 9.5129, 9.3379, 9.6714, 6.9792, 7.3228, 6.5687, 8.2878,\n",
      "        9.6981, 5.6685, 8.5283, 6.3793, 6.4700, 8.8106, 5.2791, 7.6388, 9.1712,\n",
      "        6.6923, 6.1763, 7.0325, 7.9338, 6.8610, 9.6848, 5.1131, 9.3440, 6.9211,\n",
      "        8.6767, 9.7563, 6.8366, 9.9156, 5.3648, 5.1276, 6.3230, 6.4890, 7.4237,\n",
      "        8.2834, 8.1492, 7.7373, 5.1196, 9.6722, 5.4207, 8.8056, 4.9443, 6.4207,\n",
      "        6.0860, 6.3415, 9.4581, 4.9989, 6.9550, 9.2885, 5.9089, 5.0906, 8.5342,\n",
      "        6.0529, 7.6529, 8.0525, 9.8855, 5.2242, 5.1420, 9.9738, 7.1768, 9.1875,\n",
      "        7.1867, 6.8961, 9.3685, 8.1529, 8.7443, 5.6614, 9.8268, 6.4715, 6.5141,\n",
      "        8.5027, 7.1841, 5.6643, 5.0940, 9.0828, 8.4075, 9.2875, 8.1426, 5.9461,\n",
      "        7.1386, 5.8275, 9.5603, 7.0924, 5.2487, 9.3479, 8.7062, 5.2101, 5.2872,\n",
      "        5.1388])\n",
      "Pairwise covariances:\n",
      "tensor([[[7.5902, 6.0431],\n",
      "         [6.0431, 8.1817]],\n",
      "\n",
      "        [[8.3840, 5.3052],\n",
      "         [5.3052, 7.4345]],\n",
      "\n",
      "        [[7.8685, 5.7379],\n",
      "         [5.7379, 8.8801]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.3567, 3.6691],\n",
      "         [3.6691, 6.4699]],\n",
      "\n",
      "        [[5.4079, 4.8267],\n",
      "         [4.8267, 9.7577]],\n",
      "\n",
      "        [[6.2859, 5.4745],\n",
      "         [5.4745, 9.6166]]])\n"
     ]
    }
   ],
   "source": [
    "result_Sigma_pairwise = []\n",
    "result_mu = []\n",
    "time_lst = []\n",
    "n_lst = [15000, 25000, 30000, 35000, 40000, 45000]\n",
    "for n in n_lst:\n",
    "  print('-----------------------------------')\n",
    "  print('n =', n)\n",
    "  start = time()\n",
    "  print('Estimating mean...')\n",
    "  model_mu = MeanEst()\n",
    "  var_hat_lst, mu_hat_lst, _, _ = model_mu.Train(M=20000,\n",
    "                                       batch_size=20,\n",
    "                                       lamb=1000,\n",
    "                                       X=torch.tensor(X, device=device).float()[:n,:],\n",
    "                                       cond_lower=torch.tensor(list(cond_lower)),\n",
    "                                       cond_upper=torch.tensor(list(cond_upper)),\n",
    "                                       affine_transform=True,\n",
    "                                       display_per_iter=200)\n",
    "  \n",
    "  print('Estimating pairwise covariance...')\n",
    "  model_cov = CovarianceEst()\n",
    "  Sigma_hat_lst, _, _, _ = model_cov.Train(M=20000,\n",
    "                                           batch_size=20,\n",
    "                                           lamb=100,\n",
    "                                           X=torch.tensor(X, device=device).float()[:n,:],\n",
    "                                           cond_lower=torch.tensor(list(cond_lower)),\n",
    "                                           cond_upper=torch.tensor(list(cond_upper)),\n",
    "                                           init_mu=None,\n",
    "                                           init_var=None,\n",
    "                                           affine_transform=True,\n",
    "                                           display_per_iter=200)\n",
    "  end = time()\n",
    "\n",
    "\n",
    "  print('Mean:')\n",
    "  print(mu_hat_lst.reshape(-1))\n",
    "  print('Pairwise covariances:')\n",
    "  print(Sigma_hat_lst)\n",
    "  result_Sigma_pairwise.append(Sigma_hat_lst)\n",
    "  result_mu.append(mu_hat_lst.reshape(-1))\n",
    "  time_lst.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'result_mu': result_mu,\n",
    "            'result_Sigma_pairwise': result_Sigma_pairwise,\n",
    "            'X': X,\n",
    "            'time_lst': time_lst,\n",
    "            'Sigma': Sigma,\n",
    "            'mu': mu,\n",
    "            'n_lst': n_lst}, 'results-{}-remove65-new2.pk'.format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 15000\n",
      "n = 25000\n",
      "n = 30000\n",
      "n = 35000\n",
      "n = 40000\n",
      "n = 45000\n"
     ]
    }
   ],
   "source": [
    "def LearnSigma(Sigma_hat_lst, mu_hat_lst, dim, epsilon):\n",
    "    pair_idx = {}\n",
    "    k = 0\n",
    "    for i in range(dim):\n",
    "      for j in range(i+1,dim):\n",
    "        pair_idx[str(i) + str(j)] = k\n",
    "        k += 1\n",
    "\n",
    "    Sig_learned = cp.Variable((dim, dim), symmetric=True)\n",
    "    Sig_hat_ij = cp.Parameter((int(dim*(dim-1)/2), 4))\n",
    "    # The operator >> denotes matrix inequality.\n",
    "    constraints = [Sig_learned >> 0] # Positive semidefinite constraint\n",
    "    constraints += [\n",
    "        Sig_learned[:,[i,j]][[i,j],:] << (1+epsilon)*cp.reshape(Sig_hat_ij[pair_idx[str(i)+str(j)]],(2,2)) for i in range(dim) for j in range(i+1,dim) \n",
    "    ]\n",
    "    constraints += [\n",
    "        Sig_learned[:,[i,j]][[i,j],:] >> (1-epsilon)*cp.reshape(Sig_hat_ij[pair_idx[str(i)+str(j)]], (2,2)) for i in range(dim) for j in range(i+1,dim) \n",
    "    ]\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(0), constraints)\n",
    "    print(prob.is_dpp())\n",
    "    # prob.solve()\n",
    "    cvxpylayer = CvxpyLayer(prob, parameters=[Sig_hat_ij], variables=[Sig_learned])\n",
    "    print('bbb')\n",
    "    # solve the problem\n",
    "    solution, = cvxpylayer(Sigma_hat_lst.reshape(-1,4))\n",
    "    print('ccc')\n",
    "\n",
    "    # Print result.\n",
    "    # print(\"A solution X is\")\n",
    "    # print(solution)\n",
    "    return solution\n",
    "  \n",
    "def LearnSigma2(Sigma_hat_lst, dim):\n",
    "  dim = X.shape[1]\n",
    "\n",
    "  pair_idx = {}\n",
    "  k = 0\n",
    "  for i in range(dim):\n",
    "    for j in range(i+1,dim):\n",
    "      pair_idx[str(i) + str(j)] = k\n",
    "      k += 1\n",
    "\n",
    "  # Define and solve the CVXPY problem.\n",
    "  # Create a symmetric matrix variable.\n",
    "  Sigma_learned = cp.Variable((dim, dim), symmetric=True)\n",
    "  # The operator >> denotes matrix inequality.\n",
    "  constraints = [Sigma_learned >> 0] # Positive semidefinite constraint\n",
    "  L = cp.sum([cp.sum((Sigma_learned[:,[i,j]][[i,j],:] - Sigma_hat_lst[pair_idx[str(i)+str(j)]])**2) for i in range(dim) for j in range(i+1,dim)])\n",
    "  prob = cp.Problem(cp.Minimize(L), constraints)\n",
    "  prob.solve()\n",
    "\n",
    "  # Print result.\n",
    "#   print(\"The optimal value is\", prob.value)\n",
    "#   print(\"A solution is\")\n",
    "#   print(Sigma_learned.value)\n",
    "  return torch.tensor(Sigma_learned.value)\n",
    "\n",
    "result_Sigma = []\n",
    "# epsilon = [0.45, 0.25, 0.2, 0.15]\n",
    "epsilon = [0.7, 0.2, 0.17, 0.15]\n",
    "for i in range(len(n_lst)):\n",
    "  print('n =', n_lst[i])\n",
    "  Sigma_hat_lst = result_Sigma_pairwise[i]\n",
    "  mu_hat_lst = result_mu[i]\n",
    "#   Sigma_learned = LearnSigma2(Sigma_hat_lst, X.shape[1], epsilon[i])\n",
    "  Sigma_learned = LearnSigma2(Sigma_hat_lst.cpu().numpy(), X.shape[1])\n",
    "  result_Sigma.append(Sigma_learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'result_mu': result_mu,\n",
    "            'result_Sigma': result_Sigma,\n",
    "            'result_Sigma_pairwise': result_Sigma_pairwise,\n",
    "            'X': X,\n",
    "            'time_lst': time_lst,\n",
    "            'Sigma': Sigma,\n",
    "            'mu': mu,\n",
    "            'n_lst': n_lst}, 'results-{}-remove65-new2.pk'.format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load1 = torch.load('results-{}-remove65-new.pk'.format(100))\n",
    "load2 = torch.load('results-{}-remove65-new2.pk'.format(100))\n",
    "\n",
    "result_mu = torch.stack((load1['result_mu'] + load2['result_mu']))\n",
    "result_Sigma_pairwise = torch.stack((load1['result_Sigma_pairwise'] + load2['result_Sigma_pairwise']))\n",
    "result_Sigma = torch.stack((load1['result_Sigma'] + load2['result_Sigma']))\n",
    "time_lst = np.asarray(load1['time_lst'] + load2['time_lst'])\n",
    "n_lst = np.asarray(load1['n_lst'] + load2['n_lst'])\n",
    "\n",
    "idx = np.argsort(n_lst)\n",
    "\n",
    "result_mu = result_mu[idx]\n",
    "result_Sigma_pairwise = result_Sigma_pairwise[idx]\n",
    "result_Sigma = result_Sigma[idx]\n",
    "time_lst = time_lst[idx]\n",
    "n_lst = n_lst[idx]\n",
    "\n",
    "X = load1['X']\n",
    "Sigma = load1['Sigma']\n",
    "mu = load1['mu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.905449345093043 0.7774765536930961 0.8519858984712535\n",
      "13.352979270864266 0.6545894469235738 0.6012479122892048\n",
      "11.987274262810972 0.5913541022098397 0.4886088096428944\n",
      "7.9043719015395215 0.3880607837765361 0.23376463541031725\n",
      "6.588494612384677 0.3228127978448632 0.16279366237326484\n",
      "6.012755522503799 0.2946246443940897 0.13778703757252425\n",
      "5.876957085863268 0.28832466861111844 0.13433493940104507\n",
      "5.454918639712352 0.26726455959669126 0.1139911243031694\n",
      "5.072421820397955 0.24836297931873633 0.09770029180560691\n",
      "4.878623570414977 0.23870127551789902 0.09022145718361012\n"
     ]
    }
   ],
   "source": [
    "mape_Sigma_lst = []\n",
    "rmse_Sigma_lst = []\n",
    "mae_Sigma_lst = []\n",
    "mape_mu_lst = []\n",
    "rmse_mu_lst = []\n",
    "mae_mu_lst = []\n",
    "for i in range(len(n_lst)):\n",
    "  mape_Sigma = np.mean(np.abs((Sigma - result_Sigma[i].cpu().numpy())/Sigma))*100\n",
    "  mape_mu = np.mean(np.abs((mu - result_mu[i].cpu().numpy())/Sigma))*100\n",
    "  \n",
    "  mae_Sigma = np.mean(np.abs(Sigma - result_Sigma[i].cpu().numpy()))\n",
    "  mae_mu = np.mean(np.abs(mu - result_mu[i].cpu().numpy()))\n",
    "  \n",
    "  rmse_Sigma = np.mean(np.abs(Sigma - result_Sigma[i].cpu().numpy())**2)\n",
    "  rmse_mu = np.mean(np.abs(mu - result_mu[i].cpu().numpy())**2)\n",
    "  \n",
    "  mape_Sigma_lst.append(mape_Sigma)\n",
    "  mape_mu_lst.append(mape_mu)\n",
    "  mae_Sigma_lst.append(mae_Sigma)\n",
    "  mae_mu_lst.append(mae_mu)\n",
    "  rmse_Sigma_lst.append(rmse_Sigma)\n",
    "  rmse_mu_lst.append(rmse_mu)\n",
    "  \n",
    "  print(mape_Sigma, mae_Sigma, rmse_Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61644645,  0.50244778,  0.66061649,  0.80029348,  1.10087484,\n",
       "         0.93146421,  0.96282839,  0.7056296 ,  0.887881  ,  0.42974502,\n",
       "         1.00918039,  0.68678624,  0.63969267,  0.6814779 ,  1.16058718,\n",
       "         1.09515512,  0.62586945,  0.90666693,  0.67506975,  0.57582528,\n",
       "         1.02143883,  0.94400806,  0.689573  ,  1.17422137,  0.57547368,\n",
       "         0.90189054,  0.92054425,  0.89926079,  1.13760156,  0.72809659],\n",
       "       [ 0.50244778,  0.31444629,  0.98207305,  0.87189889,  0.68728191,\n",
       "         0.46373662,  0.55291487,  0.55575302,  0.45182334,  0.71504171,\n",
       "         0.2838957 ,  0.55892644,  0.90105888,  0.59313177,  0.90404852,\n",
       "         0.96068037,  0.26768459,  1.05472235,  0.55873778,  0.9004473 ,\n",
       "         0.75730737,  0.68596705,  0.65142485,  0.48822782,  0.45646513,\n",
       "         0.68238017,  0.74183255,  0.94957508,  1.15116259,  0.69467789],\n",
       "       [ 0.66061649,  0.98207305,  0.10786863,  0.77256559,  0.82603874,\n",
       "         0.5636526 ,  0.42396458,  0.52983942,  0.3405251 ,  0.56905843,\n",
       "         0.71617123,  0.29934712,  0.90284222,  0.2656439 ,  0.7480839 ,\n",
       "         0.38477168,  0.42896223,  0.22072155,  0.67130066,  0.46411786,\n",
       "         0.61351391,  0.57749992,  0.52747743,  0.49030468,  0.46037251,\n",
       "         0.54393165,  1.01710146,  0.77773504,  0.84979215,  0.57102634],\n",
       "       [ 0.80029348,  0.87189889,  0.77256559, -0.08648217,  0.69726671,\n",
       "         0.72939877,  0.6069471 ,  0.11708089,  0.50243594,  0.47945905,\n",
       "         0.43346346,  0.36387048,  0.94642793,  0.33194231,  0.56337775,\n",
       "         0.69053317,  0.45064617,  0.73911486,  0.41025138,  0.30500397,\n",
       "         0.75343891,  0.61110805,  0.34022772,  0.47358889,  0.24639977,\n",
       "         0.62000634,  0.62360292,  0.62768783,  0.77264589,  0.63171848],\n",
       "       [ 1.10087484,  0.68728191,  0.82603874,  0.69726671,  0.09700996,\n",
       "         0.73293198,  1.01629083,  0.88402835,  0.43032117,  0.90087975,\n",
       "         0.79062548,  0.48045874,  1.02309854,  0.43339449,  1.05181252,\n",
       "         1.00743626,  0.68234438,  0.56278082,  0.54763769,  0.95971407,\n",
       "         0.81442789,  0.59207681,  0.81536235,  0.90911438,  1.09041228,\n",
       "         0.67937823,  0.66680624,  1.00499707,  0.20392903,  0.6137948 ],\n",
       "       [ 0.93146421,  0.46373662,  0.5636526 ,  0.72939877,  0.73293198,\n",
       "         0.34717881,  0.55600834,  0.26662139,  0.7243806 ,  0.40848015,\n",
       "         1.02325904,  0.33699392,  1.42579163,  0.4810787 ,  0.75667229,\n",
       "         0.83407595,  1.05100259,  0.74637463,  1.01630294,  0.72104948,\n",
       "         0.76033444,  0.83867638,  1.12926719,  1.13042609,  0.95495603,\n",
       "         0.3422433 ,  0.79819347,  0.48539963,  0.77899755,  0.80287188],\n",
       "       [ 0.96282839,  0.55291487,  0.42396458,  0.6069471 ,  1.01629083,\n",
       "         0.55600834,  0.43485778,  0.895091  ,  0.30712585,  1.16415949,\n",
       "         0.68532133,  0.37097377,  0.66124299,  0.64589212,  0.67451452,\n",
       "         0.4965212 ,  0.74418712,  0.96755589,  0.72445393,  0.78048027,\n",
       "         0.539166  ,  0.95600525,  0.83820309,  0.84517629,  0.94944201,\n",
       "         0.6099592 ,  0.70509315,  0.74207088,  0.99571868,  0.62452121],\n",
       "       [ 0.7056296 ,  0.55575302,  0.52983942,  0.11708089,  0.88402835,\n",
       "         0.26662139,  0.895091  , -0.18017621,  0.40779583,  1.02448183,\n",
       "         0.8061584 ,  0.55646657,  0.73169816,  0.75369771,  0.90717923,\n",
       "         0.16049472,  0.73937458,  0.39119946,  0.93026789,  0.29658119,\n",
       "         0.75050305,  0.46107263,  0.44622241,  0.7847785 ,  0.71829787,\n",
       "         0.64573997,  0.76061297,  0.65712998,  0.45814543,  0.95725691],\n",
       "       [ 0.887881  ,  0.45182334,  0.3405251 ,  0.50243594,  0.43032117,\n",
       "         0.7243806 ,  0.30712585,  0.40779583, -0.25801558,  0.64773441,\n",
       "         0.73860433,  0.48148599,  1.22899617,  0.43432678,  0.82104473,\n",
       "         0.59363871,  0.4344987 ,  0.80804429,  0.41016061,  0.60246385,\n",
       "         0.77966636,  0.77719646,  0.60819576,  0.4010808 ,  0.30620383,\n",
       "         0.68689294,  0.53418671,  0.49077894,  0.75402726,  0.77957135],\n",
       "       [ 0.42974502,  0.71504171,  0.56905843,  0.47945905,  0.90087975,\n",
       "         0.40848015,  1.16415949,  1.02448183,  0.64773441,  0.30018554,\n",
       "         0.94574537,  0.30933867, -0.07557376,  0.67887505,  0.99947024,\n",
       "         0.65175651,  0.84246503,  1.05137005,  1.15714558,  0.82953018,\n",
       "         0.98675743,  0.77178668,  0.4910945 ,  0.79697948,  0.51269533,\n",
       "         0.76351667,  0.64265092,  1.07821234, -0.28471154,  0.77849201],\n",
       "       [ 1.00918039,  0.2838957 ,  0.71617123,  0.43346346,  0.79062548,\n",
       "         1.02325904,  0.68532133,  0.8061584 ,  0.73860433,  0.94574537,\n",
       "         0.56350683,  0.64565208,  0.93365892,  0.84004139,  0.56313042,\n",
       "         0.72673312,  0.84756132,  0.92765462,  0.75531528,  0.76937635,\n",
       "         0.94552496,  0.91588015,  0.72237909,  0.80198616,  1.06491278,\n",
       "         0.34767343,  0.55775944,  0.82504372,  0.78652688,  0.89369491],\n",
       "       [ 0.68678624,  0.55892644,  0.29934712,  0.36387048,  0.48045874,\n",
       "         0.33699392,  0.37097377,  0.55646657,  0.48148599,  0.30933867,\n",
       "         0.64565208,  0.24840837,  0.28557291,  0.40395179,  0.87868168,\n",
       "         0.64239924,  0.42586941,  0.58382281,  0.83770992,  0.22854862,\n",
       "         0.45502911,  0.35678012,  0.24239749,  0.46210165,  0.34901778,\n",
       "         0.28264155,  0.60099381,  0.66146247,  0.75604382,  0.80022138],\n",
       "       [ 0.63969267,  0.90105888,  0.90284222,  0.94642793,  1.02309854,\n",
       "         1.42579163,  0.66124299,  0.73169816,  1.22899617, -0.07557376,\n",
       "         0.93365892,  0.28557291,  0.55326078,  0.58036982,  1.27018217,\n",
       "         0.96243312,  1.14448429,  1.14488879,  1.12538185,  0.96982651,\n",
       "         1.04888048,  0.45432099,  1.13036327,  0.99300063,  0.96438712,\n",
       "         0.9567401 ,  0.88783474,  1.00596694,  1.05917097,  0.52861715],\n",
       "       [ 0.6814779 ,  0.59313177,  0.2656439 ,  0.33194231,  0.43339449,\n",
       "         0.4810787 ,  0.64589212,  0.75369771,  0.43432678,  0.67887505,\n",
       "         0.84004139,  0.40395179,  0.58036982,  0.22958453,  0.73860435,\n",
       "         0.70739531,  0.78716564,  0.59960408,  0.69123374,  0.53148642,\n",
       "         0.70100908,  0.57760832,  0.60959744,  0.91576066,  0.30755853,\n",
       "         0.63019576,  0.58344574,  0.73666226,  0.45134597,  0.68716027],\n",
       "       [ 1.16058718,  0.90404852,  0.7480839 ,  0.56337775,  1.05181252,\n",
       "         0.75667229,  0.67451452,  0.90717923,  0.82104473,  0.99947024,\n",
       "         0.56313042,  0.87868168,  1.27018217,  0.73860435,  0.16406797,\n",
       "         0.35758006,  0.73957155,  0.73812826,  1.15407833,  0.74677671,\n",
       "         0.58790092,  0.81201899,  1.02383026,  0.14524913,  0.74702209,\n",
       "         0.87764114,  0.70423828,  0.40487333,  0.6300279 ,  0.64918661],\n",
       "       [ 1.09515512,  0.96068037,  0.38477168,  0.69053317,  1.00743626,\n",
       "         0.83407595,  0.4965212 ,  0.16049472,  0.59363871,  0.65175651,\n",
       "         0.72673312,  0.64239924,  0.96243312,  0.70739531,  0.35758006,\n",
       "         0.43876118,  0.69393711,  0.77728311,  1.08714013,  0.95090234,\n",
       "         0.47168517,  0.79034718,  0.63688268,  0.39828638,  0.58583259,\n",
       "         0.88797103,  0.86583919,  0.6888172 ,  0.88945692,  0.73158189],\n",
       "       [ 0.62586945,  0.26768459,  0.42896223,  0.45064617,  0.68234438,\n",
       "         1.05100259,  0.74418712,  0.73937458,  0.4344987 ,  0.84246503,\n",
       "         0.84756132,  0.42586941,  1.14448429,  0.78716564,  0.73957155,\n",
       "         0.69393711,  0.03161856,  0.6407255 ,  0.93949241,  0.88023933,\n",
       "         0.81341361,  0.54578591,  0.66152583,  0.89207369,  0.8684628 ,\n",
       "         0.47048558,  0.50192021, -0.14472874,  0.93716881,  0.26554636],\n",
       "       [ 0.90666693,  1.05472235,  0.22072155,  0.73911486,  0.56278082,\n",
       "         0.74637463,  0.96755589,  0.39119946,  0.80804429,  1.05137005,\n",
       "         0.92765462,  0.58382281,  1.14488879,  0.59960408,  0.73812826,\n",
       "         0.77728311,  0.6407255 ,  0.44841146,  1.25378824,  0.58777722,\n",
       "         1.03891688,  0.89564738,  0.83362544,  0.99536324,  1.14635476,\n",
       "         0.92613114,  0.8000776 ,  1.2534059 ,  0.89990633,  1.09161345],\n",
       "       [ 0.67506975,  0.55873778,  0.67130066,  0.41025138,  0.54763769,\n",
       "         1.01630294,  0.72445393,  0.93026789,  0.41016061,  1.15714558,\n",
       "         0.75531528,  0.83770992,  1.12538185,  0.69123374,  1.15407833,\n",
       "         1.08714013,  0.93949241,  1.25378824,  0.51097274,  0.45652447,\n",
       "         1.21562807,  0.87714863,  0.70565426,  0.70421882,  0.81184136,\n",
       "         0.15692477,  0.58498061,  0.76229663,  0.55219716,  0.79917611],\n",
       "       [ 0.57582528,  0.9004473 ,  0.46411786,  0.30500397,  0.95971407,\n",
       "         0.72104948,  0.78048027,  0.29658119,  0.60246385,  0.82953018,\n",
       "         0.76937635,  0.22854862,  0.96982651,  0.53148642,  0.74677671,\n",
       "         0.95090234,  0.88023933,  0.58777722,  0.45652447, -0.04244679,\n",
       "         0.67898808,  0.69880821,  0.20977649,  0.45287161,  0.44063146,\n",
       "         1.01796963,  0.82737454,  0.0624377 ,  0.75060452,  0.912846  ],\n",
       "       [ 1.02143883,  0.75730737,  0.61351391,  0.75343891,  0.81442789,\n",
       "         0.76033444,  0.539166  ,  0.75050305,  0.77966636,  0.98675743,\n",
       "         0.94552496,  0.45502911,  1.04888048,  0.70100908,  0.58790092,\n",
       "         0.47168517,  0.81341361,  1.03891688,  1.21562807,  0.67898808,\n",
       "         0.4104778 ,  0.96012292,  0.57842072,  0.83545977,  0.80141944,\n",
       "         0.36664501,  0.68499955,  0.75567819,  0.49104465,  1.07908333],\n",
       "       [ 0.94400806,  0.68596705,  0.57749992,  0.61110805,  0.59207681,\n",
       "         0.83867638,  0.95600525,  0.46107263,  0.77719646,  0.77178668,\n",
       "         0.91588015,  0.35678012,  0.45432099,  0.57760832,  0.81201899,\n",
       "         0.79034718,  0.54578591,  0.89564738,  0.87714863,  0.69880821,\n",
       "         0.96012292,  0.27127776,  0.7420865 ,  0.46904439,  0.75371792,\n",
       "         0.66550271,  0.93123361,  0.52270424,  0.38365793,  0.75892701],\n",
       "       [ 0.689573  ,  0.65142485,  0.52747743,  0.34022772,  0.81536235,\n",
       "         1.12926719,  0.83820309,  0.44622241,  0.60819576,  0.4910945 ,\n",
       "         0.72237909,  0.24239749,  1.13036327,  0.60959744,  1.02383026,\n",
       "         0.63688268,  0.66152583,  0.83362544,  0.70565426,  0.20977649,\n",
       "         0.57842072,  0.7420865 ,  0.16192726,  0.44468587,  0.67961047,\n",
       "         0.38737483,  0.97929656,  0.58303623,  0.79288052,  0.8468701 ],\n",
       "       [ 1.17422137,  0.48822782,  0.49030468,  0.47358889,  0.90911438,\n",
       "         1.13042609,  0.84517629,  0.7847785 ,  0.4010808 ,  0.79697948,\n",
       "         0.80198616,  0.46210165,  0.99300063,  0.91576066,  0.14524913,\n",
       "         0.39828638,  0.89207369,  0.99536324,  0.70421882,  0.45287161,\n",
       "         0.83545977,  0.46904439,  0.44468587,  0.01179725,  0.73805888,\n",
       "         1.01580672,  0.46940286,  0.87765292,  0.47557373,  1.0469319 ],\n",
       "       [ 0.57547368,  0.45646513,  0.46037251,  0.24639977,  1.09041228,\n",
       "         0.95495603,  0.94944201,  0.71829787,  0.30620383,  0.51269533,\n",
       "         1.06491278,  0.34901778,  0.96438712,  0.30755853,  0.74702209,\n",
       "         0.58583259,  0.8684628 ,  1.14635476,  0.81184136,  0.44063146,\n",
       "         0.80141944,  0.75371792,  0.67961047,  0.73805888, -0.2991177 ,\n",
       "         0.59822151,  0.59190062,  0.45681906,  0.76952987,  0.67633735],\n",
       "       [ 0.90189054,  0.68238017,  0.54393165,  0.62000634,  0.67937823,\n",
       "         0.3422433 ,  0.6099592 ,  0.64573997,  0.68689294,  0.76351667,\n",
       "         0.34767343,  0.28264155,  0.9567401 ,  0.63019576,  0.87764114,\n",
       "         0.88797103,  0.47048558,  0.92613114,  0.15692477,  1.01796963,\n",
       "         0.36664501,  0.66550271,  0.38737483,  1.01580672,  0.59822151,\n",
       "         0.29331836,  0.76764497,  1.06353088,  1.08962578,  0.32123579],\n",
       "       [ 0.92054425,  0.74183255,  1.01710146,  0.62360292,  0.66680624,\n",
       "         0.79819347,  0.70509315,  0.76061297,  0.53418671,  0.64265092,\n",
       "         0.55775944,  0.60099381,  0.88783474,  0.58344574,  0.70423828,\n",
       "         0.86583919,  0.50192021,  0.8000776 ,  0.58498061,  0.82737454,\n",
       "         0.68499955,  0.93123361,  0.97929656,  0.46940286,  0.59190062,\n",
       "         0.76764497,  0.18626631,  0.80546383,  1.06100213,  0.75334938],\n",
       "       [ 0.89926079,  0.94957508,  0.77773504,  0.62768783,  1.00499707,\n",
       "         0.48539963,  0.74207088,  0.65712998,  0.49077894,  1.07821234,\n",
       "         0.82504372,  0.66146247,  1.00596694,  0.73666226,  0.40487333,\n",
       "         0.6888172 , -0.14472874,  1.2534059 ,  0.76229663,  0.0624377 ,\n",
       "         0.75567819,  0.52270424,  0.58303623,  0.87765292,  0.45681906,\n",
       "         1.06353088,  0.80546383,  0.0610853 ,  0.69944392,  0.98682075],\n",
       "       [ 1.13760156,  1.15116259,  0.84979215,  0.77264589,  0.20392903,\n",
       "         0.77899755,  0.99571868,  0.45814543,  0.75402726, -0.28471154,\n",
       "         0.78652688,  0.75604382,  1.05917097,  0.45134597,  0.6300279 ,\n",
       "         0.88945692,  0.93716881,  0.89990633,  0.55219716,  0.75060452,\n",
       "         0.49104465,  0.38365793,  0.79288052,  0.47557373,  0.76952987,\n",
       "         1.08962578,  1.06100213,  0.69944392,  0.30498118,  0.52752323],\n",
       "       [ 0.72809659,  0.69467789,  0.57102634,  0.63171848,  0.6137948 ,\n",
       "         0.80287188,  0.62452121,  0.95725691,  0.77957135,  0.77849201,\n",
       "         0.89369491,  0.80022138,  0.52861715,  0.68716027,  0.64918661,\n",
       "         0.73158189,  0.26554636,  1.09161345,  0.79917611,  0.912846  ,\n",
       "         1.07908333,  0.75892701,  0.8468701 ,  1.0469319 ,  0.67633735,\n",
       "         0.32123579,  0.75334938,  0.98682075,  0.52752323,  0.57219475]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma - result_Sigma[i].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(n_lst,rmse_Sigma_lst,marker='^')\n",
    "# plt.ylim(0,9)\n",
    "# plt.xticks(range(4),n_lst)\n",
    "plt.ylabel('The error ')\n",
    "plt.title('RMSE of $\\hat{\\Sigma}$')\n",
    "plt.xlabel('N')\n",
    "# plt.xticks(range(4),n_lst)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_lst,rmse_mu_lst)\n",
    "# plt.ylim(0,6)\n",
    "plt.ylabel('The error')\n",
    "plt.title('RMSE of $\\hat{\\mu}$')\n",
    "plt.xlabel('N')\n",
    "# plt.xticks(range(4),n_lst)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Test2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
